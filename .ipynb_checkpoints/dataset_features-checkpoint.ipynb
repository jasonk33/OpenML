{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests import get\n",
    "import json\n",
    "from scipy.stats import kurtosis, skew, entropy\n",
    "%load_ext autotime\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Workflow:\n",
    "- Functions to get dataset features as described in the paper Efficient and Robust Automated Machine Learning\n",
    "- Get dataset features using paper features\n",
    "- Get dataset features using OpenML features\n",
    "- Combine features from both methods\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "# Read in necessary data\n",
    "with open('OpenML_Data/sklearn_task_descriptions.json') as data_file:\n",
    "    sklearn_task_descriptions = json.load(data_file)\n",
    "    \n",
    "with open('OpenML_Data/sklearn_data_set_descriptions.json') as data_file:\n",
    "    sklearn_data_set_descriptions = json.load(data_file)\n",
    "    \n",
    "with open('OpenML_Data/sklearn_data_set_qualities.json') as data_file:\n",
    "    sklearn_data_set_qualities = json.load(data_file)\n",
    "    \n",
    "with open('OpenML_Data/sklearn_data_set_features.json') as data_file:\n",
    "    sklearn_data_set_features = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 412 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function to get the 37 dataset features described in the paper\n",
    "- X: the predictor variables\n",
    "- y: the response variable\n",
    "- categorical: an array of 0's or 1's for whether the predictor variable is categorical \n",
    "\"\"\"\n",
    "def get_features(X,y,categorical):\n",
    "\n",
    "    NumberOfInstances = float(X.shape[0])\n",
    "\n",
    "    LogNumberOfInstances = np.log(NumberOfInstances)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        NumberOfClasses = np.mean([len(np.unique(y[:,i])) for i in range(y.shape[1])])\n",
    "    else:\n",
    "        NumberOfClasses = float(len(np.unique(y)))\n",
    "\n",
    "    NumberOfFeatures = float(X.shape[1])\n",
    "\n",
    "    LogNumberOfFeatures = np.log(NumberOfFeatures)\n",
    "\n",
    "    missing = 0\n",
    "    missing_instances = []\n",
    "    missing_features = []\n",
    "    for row_idx, row in data.iterrows():\n",
    "        for col_idx,col in enumerate(row):\n",
    "            if \" ?\" == col:\n",
    "                missing += 1\n",
    "                missing_instances.append(row_idx)\n",
    "                missing_features.append(col_idx)\n",
    "    missing_instances = list(set(missing_instances))\n",
    "    missing_features = list(set(missing_features))\n",
    "\n",
    "    NumberOfInstancesWithMissingValues = len(missing_instances)\n",
    "\n",
    "    PercentageOfInstancesWithMissingValues = NumberOfInstancesWithMissingValues/NumberOfInstances\n",
    "\n",
    "    NumberOfFeaturesWithMissingValues = len(missing_features)\n",
    "\n",
    "    PercentageOfFeaturesWithMissingValues = NumberOfFeaturesWithMissingValues/NumberOfInstances\n",
    "\n",
    "    NumberOfMissingValues = missing\n",
    "\n",
    "    PercentageOfMissingValues = missing/(X.shape[0]*X.shape[1])\n",
    "\n",
    "    NumberOfNumericFeatures = len(categorical) - np.sum(categorical)\n",
    "\n",
    "    NumberOfCategoricalFeatures = np.sum(categorical)\n",
    "\n",
    "    if NumberOfCategoricalFeatures == 0.0:\n",
    "        RatioNumericalToNominal = 0.0\n",
    "    else:\n",
    "        RatioNumericalToNominal = NumberOfNumericFeatures / NumberOfCategoricalFeatures\n",
    "\n",
    "    if NumberOfNumericFeatures == 0.0:\n",
    "        RatioNominalToNumerical = 0.0\n",
    "    else:\n",
    "        RatioNominalToNumerical = NumberOfCategoricalFeatures / NumberOfNumericFeatures\n",
    "\n",
    "    DatasetRatio = NumberOfFeatures / NumberOfInstances\n",
    "\n",
    "    LogDatasetRatio = np.log(DatasetRatio)\n",
    "\n",
    "    InverseDatasetRatio = NumberOfInstances / NumberOfFeatures\n",
    "\n",
    "    LogInverseDatasetRatio = np.log(InverseDatasetRatio)\n",
    "\n",
    "    occurence_dict = {}\n",
    "    for val in set(y):\n",
    "        occurence_dict[str(val)] = 0\n",
    "    for value in y:\n",
    "        occurence_dict[str(value)] += 1\n",
    "    ClassOccurences = occurence_dict   \n",
    "\n",
    "    min_value = np.iinfo(np.int64).max\n",
    "    max_value = np.iinfo(np.int64).min\n",
    "    for class_val, num_occurences in ClassOccurences.items():\n",
    "        if num_occurences < min_value:\n",
    "            min_value = num_occurences\n",
    "        if num_occurences > max_value:\n",
    "            max_value = num_occurences\n",
    "    ClassProbabilityMin = float(min_value) / float(y.shape[0])\n",
    "    ClassProbabilityMax = float(max_value) / float(y.shape[0])\n",
    "\n",
    "    occurences = np.array([occurrence for occurrence in occurence_dict.values()],dtype=np.float64)\n",
    "    ClassProbabilityMean = (occurences / y.shape[0]).mean()\n",
    "    ClassProbabilitySTD = (occurences / y.shape[0]).std()\n",
    "    \n",
    "    symbols_per_column = []\n",
    "    kurtosisses = []\n",
    "    skewnesses = []\n",
    "    col_num = 0\n",
    "    for column in X:\n",
    "        if categorical[col_num]:\n",
    "            unique_values = np.unique(X[column])\n",
    "            num_unique = len(unique_values)\n",
    "            symbols_per_column.append(num_unique)\n",
    "        else:\n",
    "            kurtosis_val = kurtosis(X[column])\n",
    "            kurtosisses.append(kurtosis_val)\n",
    "            skewness_val = skew(X[column])\n",
    "            skewnesses.append(skewness_val)\n",
    "        col_num += 1\n",
    "        \n",
    "    if len(symbols_per_column) == 0:\n",
    "        SymbolsMin = 0\n",
    "        SymbolsMax = 0\n",
    "        SymbolsMean = 0\n",
    "        SymbolsSTD = 0\n",
    "        SymbolsSum = 0\n",
    "        KurtosisMin = 0\n",
    "        KurtosisMax = 0\n",
    "        KurtosisMean = 0\n",
    "        KurtosisSTD = 0\n",
    "        SkewnessMin = 0\n",
    "        SkewnessMax = 0\n",
    "        SkewnessMean = 0\n",
    "        SkewnessSTD = 0\n",
    "    else:\n",
    "        SymbolsMin = min(symbols_per_column)\n",
    "        SymbolsMax = max(symbols_per_column)\n",
    "        SymbolsMean = np.mean(symbols_per_column)\n",
    "        SymbolsSTD = np.std(symbols_per_column)\n",
    "        SymbolsSum = sum(symbols_per_column)\n",
    "        KurtosisMin = min(kurtosisses)\n",
    "        KurtosisMax = max(kurtosisses)\n",
    "        KurtosisMean = np.mean(kurtosisses)\n",
    "        KurtosisSTD = np.std(kurtosisses)\n",
    "        SkewnessMin = min(skewnesses)\n",
    "        SkewnessMax = max(skewnesses)\n",
    "        SkewnessMean = np.mean(skewnesses)\n",
    "        SkewnessSTD = np.std(skewnesses)\n",
    "    \n",
    "\n",
    "    entropies = []\n",
    "    occurence_dict = {}\n",
    "    for value in y:\n",
    "        if value in occurence_dict:\n",
    "            occurence_dict[value] += 1\n",
    "        else:\n",
    "            occurence_dict[value] = 1\n",
    "    ClassEntropy = entropy([occurence_dict[key] for key in occurence_dict], base=2)\n",
    "\n",
    "    \n",
    "    \n",
    "    col_names = [\"NumberOfInstances\",\"LogNumberOfInstances\",\"NumberOfClasses\",\"NumberOfFeatures\",\"LogNumberOfFeatures\",\n",
    "                                     \"NumberOfInstancesWithMissingValues\",\"PercentageOfInstancesWithMissingValues\",\"NumberOfFeaturesWithMissingValues\",\n",
    "                                     \"PercentageOfFeaturesWithMissingValues\",\"NumberOfMissingValues\",\"PercentageOfMissingValues\",\"NumberOfNumericFeatures\",\n",
    "                                     \"NumberOfCategoricalFeatures\",\"RatioNumericalToNominal\",\"RatioNominalToNumerical\",\"DatasetRatio\",\"LogDatasetRatio\",\n",
    "                                     \"InverseDatasetRatio\",\"LogInverseDatasetRatio\",\"ClassProbabilityMin\",\"ClassProbabilityMax\", \"ClassProbabilityMean\",\n",
    "                                     \"ClassProbabilitySTD\", \"SymbolsMin\", \"SymbolsMax\", \"SymbolsMean\", \"SymbolsSTD\", \"SymbolsSum\", \"KurtosisMin\", \n",
    "                 \"KurtosisMax\", \"KurtosisMean\", \"KurtosisSTD\", \"SkewnessMin\", \"SkewnessMax\", \"SkewnessMean\", \"SkewnessSTD\", \"ClassEntropy\"]\n",
    "    \n",
    "    return pd.Series([NumberOfInstances,LogNumberOfInstances,NumberOfClasses,NumberOfFeatures,LogNumberOfFeatures,\n",
    "                               NumberOfInstancesWithMissingValues,PercentageOfInstancesWithMissingValues,NumberOfFeaturesWithMissingValues,\n",
    "                               PercentageOfFeaturesWithMissingValues,NumberOfMissingValues,PercentageOfMissingValues,NumberOfNumericFeatures,\n",
    "                               NumberOfCategoricalFeatures,RatioNumericalToNominal,RatioNominalToNumerical,DatasetRatio,LogDatasetRatio,\n",
    "                               InverseDatasetRatio,LogInverseDatasetRatio,ClassProbabilityMin,ClassProbabilityMax,ClassProbabilityMean,\n",
    "                               ClassProbabilitySTD, SymbolsMin, SymbolsMax, SymbolsMean, SymbolsSTD, SymbolsSum, KurtosisMin, KurtosisMax,\n",
    "                     KurtosisMean, KurtosisSTD, SkewnessMin, SkewnessMax, SkewnessMean, SkewnessSTD, ClassEntropy], index=col_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.07 ms\n"
     ]
    }
   ],
   "source": [
    "# Function to get the X, y, and categorical variables for the \"get_features\" function from an OpenML dataset\n",
    "def get_data_base(features,data):\n",
    "    categorical = []\n",
    "    for column in features['data_features']['feature']:\n",
    "        if column['data_type'] == 'numeric':\n",
    "            categorical.append(0)\n",
    "        else:\n",
    "            categorical.append(1)\n",
    "        if column['is_target'] == 'true':\n",
    "            X = data.drop(data.columns[int(column['index'])], axis=1)\n",
    "            y = data.iloc[:,int(column['index'])]\n",
    "    return X,y,categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.5 ms\n"
     ]
    }
   ],
   "source": [
    "# Get all sklearn dataset ids and file ids\n",
    "data_set_ids = []\n",
    "data_set_file_ids = {}\n",
    "for task_id,task_description in sklearn_task_descriptions.items():\n",
    "    data_set_id = task_description['task']['input'][0]['data_set']['data_set_id']\n",
    "    data_set_file_id = sklearn_data_set_descriptions[data_set_id]['data_set_description']['file_id']\n",
    "    data_set_ids.append(data_set_id)\n",
    "    data_set_file_ids[data_set_id] = data_set_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.87 ms\n"
     ]
    }
   ],
   "source": [
    "# Feature names  \n",
    "    col_names = [\"NumberOfInstances\",\"LogNumberOfInstances\",\"NumberOfClasses\",\"NumberOfFeatures\",\"LogNumberOfFeatures\",\n",
    "                                     \"NumberOfInstancesWithMissingValues\",\"PercentageOfInstancesWithMissingValues\",\"NumberOfFeaturesWithMissingValues\",\n",
    "                                     \"PercentageOfFeaturesWithMissingValues\",\"NumberOfMissingValues\",\"PercentageOfMissingValues\",\"NumberOfNumericFeatures\",\n",
    "                                     \"NumberOfCategoricalFeatures\",\"RatioNumericalToNominal\",\"RatioNominalToNumerical\",\"DatasetRatio\",\"LogDatasetRatio\",\n",
    "                                     \"InverseDatasetRatio\",\"LogInverseDatasetRatio\",\"ClassProbabilityMin\",\"ClassProbabilityMax\", \"ClassProbabilityMean\",\n",
    "                                     \"ClassProbabilitySTD\", \"SymbolsMin\", \"SymbolsMax\", \"SymbolsMean\", \"SymbolsSTD\", \"SymbolsSum\", \"KurtosisMin\", \n",
    "                 \"KurtosisMax\", \"KurtosisMean\", \"KurtosisSTD\", \"SkewnessMin\", \"SkewnessMax\", \"SkewnessMean\", \"SkewnessSTD\", \"ClassEntropy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset features for all OpenML sklearn datasets (1 Hour)\n",
    "error_data_sets = []\n",
    "df_data_sets = pd.DataFrame(columns=col_names)\n",
    "for idx, data_set_id in enumerate(data_set_ids):\n",
    "    try:\n",
    "        for quality in sklearn_data_set_qualities[data_set_id]['data_qualities']['quality']:\n",
    "            if quality['name'] == 'NumberOfInstances':\n",
    "                instances = int(float(quality['value']))\n",
    "            if quality['name'] == 'NumberOfFeatures':\n",
    "                features = int(float(quality['value']))\n",
    "        if instances * features < 6900000:\n",
    "            url='https://www.openml.org/data/get_csv/' + str(data_set_file_ids[data_set_id])\n",
    "            response = get(url).content\n",
    "            with open(\"temp.csv\", 'wb') as f:\n",
    "                f.write(response)\n",
    "\n",
    "            data = pd.read_csv(\"temp.csv\")\n",
    "\n",
    "            features = sklearn_data_set_features[data_set_id]\n",
    "\n",
    "            X,y,categorical = get_data_base(features,data)\n",
    "\n",
    "            data_set_features = get_features(X,y,categorical)\n",
    "\n",
    "            data_set_features.name = data_set_id\n",
    "\n",
    "            df_data_sets = df_data_sets.append(data_set_features)\n",
    "\n",
    "    except:\n",
    "        error_data_sets.append(data_set_id)\n",
    "        \n",
    "# df_data_sets.to_csv(\"OpenML_Data/paper_data_set_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "# Get all the features given by OpenML\n",
    "df_indices = [str(id) for id in list(df_data_sets.index)]\n",
    "openml_features = pd.DataFrame()\n",
    "for data_set_id in df_indices:\n",
    "    quality_values = pd.Series()\n",
    "    for quality in sklearn_data_set_qualities[data_set_id]['data_qualities']['quality']:\n",
    "        quality_values = quality_values.append(pd.Series(quality['value'], index=[quality['name']]))\n",
    "        quality_values.name = data_set_id\n",
    "    openml_features = openml_features.append(quality_values, ignore_index=False)\n",
    "    \n",
    "#openml_features.to_csv(\"OpenML_Data/openml_data_set_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.2 ms\n"
     ]
    }
   ],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    return [value for value in lst1 if value in lst2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.6 ms\n"
     ]
    }
   ],
   "source": [
    "# Get the combined features from OpenML and the paper\n",
    "openml_features.dropna(axis=1, how='any')\n",
    "same_cols = intersection(list(df_data_sets.columns),list(openml_features.columns))\n",
    "combined_data_set_features = pd.concat([df_data_sets, openml_features.drop(same_cols, axis=1)], axis=1)\n",
    "# combined_data_set_features.to_csv(\"OpenML_Data/combined_data_set_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.14 ms\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
